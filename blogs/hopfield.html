<!DOCTYPE html>

<html lang="en">

  <head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-BX8JFPKXJ4"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-BX8JFPKXJ4');
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=0.8">
    <meta name="author" content="Aayush Rath">
    <meta name="description" content="A website to write my blogs and my projects">
    <title>Aayush Rath</title>
    <link rel="icon" href="../assets/images/screw-removebg-preview.png">
    <link href='https://fonts.googleapis.com/css?family=Josefin Slab' rel='stylesheet'>
    <link href='https://fonts.googleapis.com/css?family=Montserrat' rel='stylesheet'>
    <link href='https://fonts.googleapis.com/css?family=Jura' rel='stylesheet'>
    <script type="importmap">
      {
        "imports": {
          "three": "https://cdn.jsdelivr.net/npm/three@0.160.1/build/three.module.js",
          "three/addons/": "https://cdn.jsdelivr.net/npm/three@0.160.1/examples/jsm/"
        }
      }
    </script>
    <style>
      * {
          box-sizing: border-box;
      }

      html{
          font-size: 22px;
          scroll-behavior: smooth;
      }


      h1 {
          text-align: center;
          font-family: 'Montserrat';
          margin-bottom: 70px;
          padding-top: 30px;
          font-weight: 100;
      }

      h2 {
        font-weight: 500;
        padding-left: 5%;
        margin-top: 30px;
        font-family: 'Montserrat', sans-serif;
      }

      .horizontal_line {
          width: 100%;
          border-top: 1px solid black;
          animation: growLine 1.2s ease-out forwards;
      }

      body.dark-mode .horizontal_line {
        border-top: 1px solid white;
      }

      .site-footer {
          text-align: center;
          margin-top: 30px;
          padding: 20px;
          font-family: 'Josefin Slab', serif;
          color: #333;
          background-color: #fffaf4;
      }

      .footer-line {
          border: none;
          border-top: 1px solid #aaa;
          margin: 40px auto 20px auto;
          width: 80%;
      }

      body.dark-mode .site-footer{
        background-color: #222222;
        color: white;
      }

      .home-button {
        position: absolute;
        top: 15px;
        right: 30px;
        background-color: #fffaf4;
        color: #222;
        padding: 6px 12px;
        border-radius: 6px;
        transition: background-color 0.3s, transform 0.2s;
        z-index: 10;
        margin-right: 50px;
        margin-top: 10px;
      }

      .home-button:hover {
        box-shadow: 0 12px 16px rgba(0, 0, 0, 0.24), 0 17px 50px rgba(0, 0, 0, 0.19);;
      }

      @keyframes growLine {
          from {
          width: 0;
          }
          to {
          width: 100%;
          }
      }

      body {
        margin: 0;
        padding: 0;
        overflow-x: hidden;
        overflow-y: auto;
        background-color: #fffaf4;
      }

      body.dark-mode {
        background-color: #222222;
        color: #f5f5f5;
      }

      .dark-mode-toggle {
        position: fixed;
        width: 50px;
        height: 50px;
        font-size: 20px;
        top: 15px;
        left: 30px;
        padding: 6px;
        border: none;
        border-radius: 6px;
        background-color: #f6ddd0;
        color: #000;
        cursor: pointer;
        box-shadow: 0 2px 6px rgba(0,0,0,0.2);
        transition: background-color 0.3s;

        z-index: 9999;
        pointer-events: auto;
      }

      body.dark-mode .dark-mode-toggle {
        background-color: #333;
        color: #fff;
      }

      .blog-container {
        max-width: 70%;
        margin: 100px auto 40px auto;
        padding: 30px;
        background-color: #fffaf4;
        color: #222222;
        font-family: 'Georgia', 'Times New Roman', serif;
        line-height: 1.7;
        font-size: 18px;
        border-radius: 12px;
        transition: background-color 0.4s, color 0.4s;
      }

      .blog-container h2 {
        padding-left: 0;
      }

      body.dark-mode .blog-container {
          background-color: #222222;
          color: #eaeaea;
      }

      .blog-image {
        display: flex;
        justify-content: center;
        margin: 20px 0;
      }

      .blog-image img {
        max-width: 700px;
        width: 100%;
        height: auto;
        border-radius: 8px;
      }

      .reference-list {
        list-style: decimal;
        margin: 20px 0;
        padding-left: 20px;
        font-size: 15px;
        line-height: 1.6;
      }

      .reference-list li {
        margin-bottom: 12px;
      }

      .reference-list a {
        color: #0066cc;
        text-decoration: none;
      }

      .reference-list a:hover {
        text-decoration: underline;
      }

      nav.sidebar {
        position: fixed;
        top: 0;
        left: 0;
        height: 100%;
        width: 220px;
        background-color: #f6ddd0;
        padding-top: 80px;
        font-family: 'Montserrat', sans-serif;
        transition: transform 0.3s ease;
        overflow-y: auto;
        z-index: 1000;
        transform: translateX(-220px);
      }

      nav.sidebar a {
        display: block;
        padding: 12px 20px;
        color: #222;
        text-decoration: none;
        transition: background 0.2s;
      }

      nav.sidebar a:hover {
        background-color: #ddd;
      }

      body.dark-mode nav.sidebar {
        background-color: #333;
      }

      body.dark-mode nav.sidebar a {
        color: #f5f5f5;
      }

      nav.sidebar.open {
        transform: translateX(0);
      }


      .sidebar-toggle {
        position: fixed;
        top: 15px;
        left: 120px;
        background-color: #f6ddd0;
        color: #222;
        border: none;
        border-radius: 6px;
        font-size: 24px;
        padding: 8px 12px;
        cursor: pointer;
        z-index: 1100;
        box-shadow: 0 2px 6px rgba(0,0,0,0.2);
        transition: background-color 0.3s;
      }

      .sidebar-toggle:hover {
        background-color: #e0c9bc;
      }

      body.dark-mode .sidebar-toggle {
        background-color: #333;
        color: #fff;
      }


      body, .page-header, .site-footer, .button, .horizontal_line,.blog-container, h1, p, a {
        transition: background-color 0.5s ease, color 0.5s ease, border-color 0.5s ease;
      }

      #spin-grid {
        display: grid;
        grid-template-columns: repeat(10, 50px);
        gap: 25px;
        justify-content: center;
        margin: 30px 0;
      }

      .spin-cell {
        width: 50px;
        height: 50px;
        font-size: 20px;
        text-align: center;
        line-height: 50px;
        cursor: pointer;
        border: none;
        border-radius: 50%;
        user-select: none;
        transition: background-color 0.3s, transform 0.2s;
        background-color: #f0f0f0;
        box-shadow: 0 2px 6px rgba(0, 0, 0, 0.1);
      }

      .spin-cell:hover {
        background-color: #e0e0e0;
        transform: scale(1.1);
      }

      .spin-cell.up {
        background-color: #cce5ff;
        color: #0056b3;
      }

      .spin-cell.down {
        background-color: #f8d7da;
        color: #721c24;
      }

      .timeline {
        list-style: none;
        padding: 0;
        margin: 20px 0;
      }

      .timeline li {
        display: grid;
        grid-template-columns: 100px auto;
        align-items: start;
        margin-bottom: 12px;
      }

      .timeline .year {
        font-weight: bold;
        color: #444;
      }

      .timeline .event {
        margin-left: 10px;
        line-height: 1.5;
      }

      body.dark-mode .timeline .year {
        color: #fafbfc;
      }


    </style>
  </head>

  <body>
    <nav class="sidebar" id="sidebar">
      <a href="#intro">Introduction</a>
      <a href="#etym">Pseudo-Etymology</a>
      <a href="#history">History</a>
      <a href="#ising">The Ising Model</a>
      <a href="#references">References</a>
      <a href="#comments">Comments</a>
    </nav>
    <button id="darkModeToggle" class="dark-mode-toggle">üåô</button>
    <button class="sidebar-toggle" id="sidebarToggle">‚ò∞</button>
    <a href="../index.html" class="home-button">
        <img src="../assets/images/home_button.png" alt="Home" width="40px">
    </a> 
    <h1>Hopfield Networks</h1>
    <div class="horizontal_line"></div>
    <section id="intro">
      <div class="blog-container">
        <h2><b>Introduction</b></h2>
        Working in robotics naturally pulled me into the world of AI‚Äîbecause in today‚Äôs era, intelligence isn‚Äôt just about code, 
        it‚Äôs about giving machines a body to interact, learn, and act in the real world. I never followed a strict, textbook path 
        to learn machine learning; instead, I stumbled into it, explored out of curiosity, and discovered how exciting the journey could be.<br><br>

        What fascinates me most is not just how AI works today, but how these ideas first took shape‚Äîthe roots of the complex architectures 
        and learning algorithms we rely on. That curiosity eventually led me to dive into Associative Memory Models, better known as Hopfield 
        Networks‚Äîa concept that bridges neuroscience, physics, and AI in a way that feels both timeless and futuristic.<br><br>

        One fascinating detail worth mentioning is that J.J. Hopfield was jointly awarded the 2024 Nobel Prize in Physics alongside Geoffrey E. Hinton,
        recognized for their ‚Äúfoundational discoveries and inventions that enable machine learning with artificial neural networks.‚Äù<br><br>

        The ideas I‚Äôm exploring here stand on the shoulders of that legacy. They are not just the work of a single mind but the result of decades of 
        collective effort‚Äîresearchers, physicists, neuroscientists, and computer scientists‚Äîeach contributing to our understanding of how the brain 
        works and how we can model intelligence artificially. Without their breakthroughs, the journey from studying neurons to building artificial
        intelligence would not have been possible.<br><br>

        To put it simply, a Hopfield Network can be thought of as a network of binary units (like little switches that are either on or off) 
        connected by weighted links. These weights are carefully set so that the network ‚Äúremembers‚Äù certain stable patterns‚Äîlike snapshots 
        of information stored in its memory.<br><br>

        Once trained, the network can reconstruct an entire memory from just a small fragment of it. This mirrors how our own brains work: 
        for example, catching just a faint smell of a dish might instantly bring back the memory of a specific food or even the moment you
        last had it. In this way, Hopfield Networks don‚Äôt just store data‚Äîthey recreate the feeling of memory recall itself.
        <div class="blog-image">
          <img src="../assets/blog-images/Hopfield_Networks/Rene_&_bot.png" alt="Rene and Bot">
        </div>
      </div>
    </section>

    <section id=etym>
      <div class="blog-container">
        <h2><b>Pseudo-Etymology</b></h2>
        The Hopfield network in itself is described as a <i>delocalized content-addressable memory or categorizer using extensive asynchronous parallel computing</i>
        <ul>
          <li>
            <b>Delocalized</b>: The stable states (the memories) are not stored in one single spot but are spread 
            across the entire network. This means that even if part of the network is disturbed, the memory can still 
            be retrieved ‚Äî much like how human memory isn‚Äôt locked in a single neuron.
          </li>
          <li>
            <b>Content-addressable</b>:Providing just a piece or a noisy version of the memory is enough to retrieve the whole thing ‚Äî similar 
            to recognizing a song from just a few notes.
          </li>
          <li>
            <b>Categorizer</b>: The network is able to distinguish between different stored memories, effectively 
            classifying the inputs into the right ‚Äúmental folder.‚Äù
          </li>
          <li>
            <b>Extensive</b>: The capacity of the network grows with its size ‚Äî the more neurons you have, the more 
            information it can store and retrieve. It scales with the system‚Äôs resources.
          </li>
          <li>
            <b>Asynchronous</b>: Neurons don‚Äôt all update at the same time. Instead, they take turns updating 
            independently, which makes the system more stable and avoids chaotic behavior. 
            This also mirrors how biological brains often work ‚Äî not everything fires at once.
          </li>
        </ul>
      </div>
    </section>

    <section id="history">
      <div class="blog-container">
        <h2><b>History</b></h2>
        <p>Here‚Äôs a quick journey through the key milestones‚Äîthe discoveries and breakthroughs that paved the way for the creation of Hopfield Networks and beyond.</p>

        <ul class="timeline">
          <li>
            <span class="year">1920</span>
            <span class="event">The <b>Ising Model</b> is introduced to represent the state of magnetic materials using spins.</span>
          </li>
          <li>
            <span class="year">1943</span>
            <span class="event"><b>McCulloch Pitts neuron</b> came to help us build neural networks</span>
          </li>
          <li>
            <span class="year">1957</span>
            <span class="event"><b>Ronsenblatt</b> gives world the idea of perceptron and simulates it on an <i>IBM 704</i></span>
          </li>
          <li>
            <span class="year">1963</span>
            <span class="event"><b>Roy Glauber</b> extends the Ising Model by introducing <i>Glauber dynamics</i>, allowing it to adapt and evolve over time.</span>
          </li>
          <li>
            <span class="year">1971‚Äì72</span>
            <span class="event"><b>Amari and Nakano</b>(separately) propose modifying the weights of the <i>Ising Model</i> using <i>Hebbian Learning</i>, a model for associative memory.</span>
          </li>
          <li>
            <span class="year">1975</span>
            <span class="event">The <b>Sherrington‚ÄìKirkpatrick model</b> is introduced, a mean-field model for spin glasses characterized by disordered and competing interactions between spins.</span>
          </li>
          <li>
            <span class="year">1982</span>
            <span class="event">Hopfield applied the SK-model idea with <i>binary activation function</i> to study <b>Hopfield Networks</b></span>
          </li>
          <li>
            <span class="year">1984</span>
            <span class="event">The concept of Hopfield Networks was extended to <i>continuous activation functions</i> (this is what gave rise to the modern neural networks) </span>
          </li>
          <li>
            <span class="year">1985</span>
            <span class="event"><b>Hopfield and Tank</b> presented an application of Hopfield Neural Networks to solve the popular <i>Travelling Salesman Problem</i> </span>
          </li>
        </ul>

        <div class="blog-image">
          <img src="../assets/blog-images/Hopfield_Networks/Angry_asimov.png" alt="Rene and Bot">
        </div>
      </div>
    </section>


    <section id="ising" style="min-height: 100vh;">
      <div class="blog-container">
        <h2><b>Ising Model</b></h2>
        Just like some of today‚Äôs popular machine learning models‚Äîsuch as diffusion models and physics-informed neural networks 
        (PINNs)‚ÄîHopfield Networks are also grounded in physics. The goal was to create a general content-addressable memory, and in 
        his original paper, Hopfield pointed out that any physical system that naturally settles into stable configurations can serve as 
        a model.<br><br>

        One of the key inspirations was the Ising Model, introduced to describe magnetism in ferromagnetic materials. In this model, every 
        unit of the material carries a tiny ‚Äúspin‚Äù (either +1 or ‚Äì1). The overall energy of the system depends on how these spins are 
        arranged. When placed in a magnetic field, the spins tend to align in ways that lower the energy‚Äîpushing the system toward stability. <br><br>
        
        This is more than just theory‚Äîyou can experiment with it! Below, try clicking on the spins in the interactive setup to see how 
        the system‚Äôs energy shifts. Notice how each dipole (spin) only ‚Äútalks‚Äù to its neighbors, yet together they form a collective behavior. 
        That same principle of local interactions leading to global order is what Hopfield leveraged to design his networks. <br><br>
      </div>
      <div id="spin-grid"></div>
      <div class="blog-container">
        Total Energy: <span id="energy-display">0</span><br><br>
        The Ising model defines energy (without any external magnetic field) as:
        <div>
          $$ E = -\sum_{\langle i, j \rangle}J_{i, j}\sigma_{i}\sigma_{j} $$
        </div>
        <ul>
          <li>\( J_{i, j} \) is the interaction between the dipoles</li>
          <li>\( \sigma_{i} \) is the spin value (either 1 or -1)</li>
        </ul>
        The central idea behind Hopfield Networks is the concept of energy minimization. A memory is introduced as input, and 
        the network adjusts its interaction parameters so that the overall energy of the system is reduced. Stable states, which 
        represent stored memories, correspond to these minimum-energy configurations.<br><br>

        This principle effectively serves as the loss function for the network. Structurally, Hopfield Networks borrow from the 
        Ising Model: a graph of interconnected binary units (0 or 1), where the state of each unit influences its neighbors. Through these 
        interactions, the network converges toward a stable pattern that encodes the memory.<br><br>

        Shown below is a 3D plot of an energy function, where the minima correspond to memory configurations.

        <div align="center">
          <script type="module" src="../javascript/energy.js"></script>
          <canvas id="energy-plot" width="300" height="225"></canvas>
        </div>
      </div>
    </section>

    <section id="learn">
      <div class="blog-container">
        <h2><b>Learning</b></h2>
        Now that we have defined an energy-based loss function, we need a method to optimize the network parameters so that the system can learn effectively. 
        Several approaches exist, but here we focus on one of the most widely recognized methods.  
        It is important to note that Hopfield Networks employ <b>symmetric weights</b> 
        (the weight from neuron <i>i</i> to <i>j</i> is the same as from <i>j</i> to <i>i</i>) to guarantee the convergence of energy values.
        
        <h3>Hebbian Learning</h3>
        <p>
        Donald Hebb introduced the <b>Hebbian theory</b>, which describes how the simultaneous activation of two neurons strengthens the synaptic connection between them.  
        This is often summarized as: <i>‚Äúneurons that fire together, wire together.‚Äù</i>  
        In the context of Hopfield Networks, this principle is used to update the weights of the network.  
        The weight for each connection is calculated as:
        </p>
        
        <div>
          $$ w_{i,j} = \frac{1}{n} \sum_{\mu=1}^{n} \epsilon_{i}^{\mu} \, \epsilon_{j}^{\mu} $$
        </div>
        <ul>
          <li>\( w_{i,j} \) is the weight of the connection between neuron <i>i</i> and neuron <i>j</i></li>
          <li>\( n \) is the total number of stored patterns</li>
          <li>\( \mu \) is the index of a specific memory pattern</li>
          <li><b>Œµ<sub>i</sub><sup>Œº</sup></b>: the state of neuron <i>i</i> in memory pattern <i>Œº</i></li>
          <li><b>Œµ<sub>j</sub><sup>Œº</sup></b>: the state of neuron <i>j</i> in memory pattern <i>Œº</i></li>
        </ul>
        

      </div>
    </section>

    <section id="references">
      <div class="blog-container">
        <h2><b>References</b></h2>
        <ol class="reference-list">
          <li>
            Hopfield, J. J. (1982). <i>Neural networks and physical systems with emergent collective 
            computational abilities.</i> Proceedings of the National Academy of Sciences, 79(8), 2554‚Äì2558. 
            <a href="https://doi.org/10.1073/pnas.79.8.2554" target="_blank">https://doi.org/10.1073/pnas.79.8.2554</a>
          </li>

          <li>
            McCulloch, W. S., & Pitts, W. (1943). <i>A logical calculus of the ideas immanent in nervous activity.</i> 
            The Bulletin of Mathematical Biophysics, 5(4), 115‚Äì133. 
            <a href="https://doi.org/10.1007/BF02478259" target="_blank">https://doi.org/10.1007/BF02478259</a>
          </li>
        
          <li>
            Bruck, J. (1990). <i>On the convergence properties of the Hopfield model.</i> 
            Proceeings of the IEEE, 78(10), 1579‚Äì1585. 
            <a href="https://doi.org/10.1109/5.58341" target="_blank">https://doi.org/10.1109/5.58341</a>
          </li>
        
          <li>
            Storkey, A. (1997). <i>Increasing the capacity of a Hopfield network without sacrificing functionality.</i> 
            In W. Gerstner, A. Germond, M. Hasler, & J.-D. Nicoud (Eds.), 
            <i>Artificial Neural Networks ‚Äî ICANN‚Äô97</i> (Lecture Notes in Computer Science, vol 1327, pp. 451‚Äì456). 
            Springer, Berlin, Heidelberg. 
            <a href="https://doi.org/10.1007/BFb0020196" target="_blank">https://doi.org/10.1007/BFb0020196</a>
          </li>
        
          <li>
            Uykan, Z. (2020). <i>On the working principle of the Hopfield Neural Networks and its equivalence to the GADIA in optimization.</i> 
            IEEE Transactions on Neural Networks and Learning Systems, 31(9), 3294‚Äì3304. 
            <a href="https://doi.org/10.1109/TNNLS.2019.2940920" target="_blank">https://doi.org/10.1109/TNNLS.2019.2940920</a>
          </li>
        
          <li>
            Hopfield, J. J., & Tank, D. W. (1985). <i>Computing with neural circuits: A model.</i> 
            Science, 233(4764), 625‚Äì633. 
            <a href="https://doi.org/10.1126/science.3755256" target="_blank">https://doi.org/10.1126/science.3755256</a>
          </li>
        </ol>
      </div>
    </section>

    <section id="links">
      <div class="blog-container">
        <h2><b>Other links</b></h2>
        <ol class="reference-list">

        </ol>
      </div>
    </section>


    <section id="comments">
      <h2>Comments</h2>
      <script src="https://giscus.app/client.js"
          data-repo="aayush-rath/aayush-rath.github.io"
          data-repo-id="R_kgDOPGW6yA"
          data-category="Announcements"
          data-category-id="DIC_kwDOPGW6yM4CtOsv"
          data-mapping="pathname"
          data-strict="0"
          data-reactions-enabled="1"
          data-emit-metadata="0"
          data-input-position="bottom"
          data-theme="preferred_color_scheme"
          data-lang="en"
          crossorigin="anonymous"
          async>
      </script>
    </section>

    <footer class="site-footer">
      <hr class="footer-line">
      <p>&copy; 2025 Aayush Rath. All rights reserved.</p>
    </footer>

    <script>
        const toggleBtn = document.getElementById('darkModeToggle');
        
        const isDarkMode = () => document.body.classList.contains('dark-mode');
        
        function updateGiscusTheme(isDark) {
            const giscusFrame = document.querySelector('iframe.giscus-frame');
            if (!giscusFrame) return;
            const message = {
            setConfig: {
                theme: isDark ? 'dark' : 'light'
            }
            };
            giscusFrame.contentWindow.postMessage(message, 'https://giscus.app');
        }
        
        function applyDarkModePreference() {
            const enabled = localStorage.getItem('darkMode') === 'enabled';
            document.body.classList.toggle('dark-mode', enabled);
        
            toggleBtn.textContent = enabled ? 'üåû' : 'üåô';
        
            if (window.updateThreeSceneForDarkMode)
            window.updateThreeSceneForDarkMode(enabled);
        
            updateGiscusTheme(enabled);
        }
        
        toggleBtn.addEventListener('click', () => {
            const enabled = !isDarkMode();
            document.body.classList.toggle('dark-mode', enabled);
            localStorage.setItem('darkMode', enabled ? 'enabled' : 'disabled');
            toggleBtn.textContent = enabled ? 'üåû' : 'üåô';
        
            if (window.updateThreeSceneForDarkMode)
            window.updateThreeSceneForDarkMode(enabled);
        
            updateGiscusTheme(enabled);
        });
        
        document.addEventListener('sceneReady', applyDarkModePreference);
        applyDarkModePreference();

        const sidebar = document.getElementById("sidebar");
        const sidebarToggle = document.getElementById("sidebarToggle");

        sidebarToggle.addEventListener("click", () => {
          sidebar.classList.toggle("open");
        });

        document.querySelectorAll('.sidebar a').forEach(anchor => {
          anchor.addEventListener('click', function(e) {
            e.preventDefault();
            document.querySelector(this.getAttribute('href')).scrollIntoView({
              behavior: 'smooth'
            });
            sidebar.classList.remove("open");
          });
        });
    </script>      
    <script>
        const N = 6;
        let spins = Array.from({ length: N }, () => Array(N).fill(1));

        const grid = document.getElementById('spin-grid');
        const energyDisplay = document.getElementById('energy-display');

        function createGrid() {
            grid.innerHTML = '';
            grid.style.gridTemplateColumns = `repeat(${N}, 35px)`;
            for (let i = 0; i < N; i++) {
                for (let j = 0; j < N; j++) {
                    const cell = document.createElement('div');
                    cell.classList.add('spin-cell', spins[i][j] === 1 ? 'up' : 'down');
                    cell.dataset.row = i;
                    cell.dataset.col = j;
                    cell.textContent = spins[i][j] === 1 ? '‚Üë' : '‚Üì';
                    cell.addEventListener('click', toggleSpin);
                    grid.appendChild(cell);
                }
            }
            updateEnergy();
        }

        function toggleSpin(e) {
            const i = parseInt(e.target.dataset.row);
            const j = parseInt(e.target.dataset.col);
            spins[i][j] *= -1;
            createGrid();
        }

        function updateEnergy() {
            let E = 0;
            for (let i = 0; i < N; i++) {
                for (let j = 0; j < N; j++) {
                    const s = spins[i][j];
                    if (i > 0) E -= s * spins[i - 1][j];
                    if (j > 0) E -= s * spins[i][j - 1];
                    if (i < N - 1) E -= s * spins[i + 1][j];
                    if (j < N - 1) E -= s * spins[i][j + 1];
                }
            }
            energyDisplay.textContent = E / 2;
        }

        createGrid();
    </script>
  </body>
</html>