<!DOCTYPE html>

<html lang="en">

  <head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-BX8JFPKXJ4"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-BX8JFPKXJ4');
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=0.8">
    <meta name="author" content="Aayush Rath">
    <meta name="description" content="A website to write my blogs and my projects">
    <title>Aayush Rath</title>
    <link rel="icon" href="../assets/images/screw-removebg-preview.png">
    <link href='https://fonts.googleapis.com/css?family=Josefin Slab' rel='stylesheet'>
    <link href='https://fonts.googleapis.com/css?family=Montserrat' rel='stylesheet'>
    <link href='https://fonts.googleapis.com/css?family=Jura' rel='stylesheet'>
    <script type="importmap">
      {
        "imports": {
          "three": "https://cdn.jsdelivr.net/npm/three@0.160.1/build/three.module.js",
          "three/addons/": "https://cdn.jsdelivr.net/npm/three@0.160.1/examples/jsm/"
        }
      }
    </script>
    <style>
      * {
          box-sizing: border-box;
      }

      html{
          font-size: 22px;
          scroll-behavior: smooth;
      }


      h1 {
          text-align: center;
          font-family: 'Montserrat';
          margin-bottom: 70px;
          padding-top: 30px;
          font-weight: 100;
      }

      h2 {
        font-weight: 500;
        padding-left: 5%;
        margin-top: 30px;
        font-family: 'Montserrat', sans-serif;
      }

      .horizontal_line {
          width: 100%;
          border-top: 1px solid black;
          animation: growLine 1.2s ease-out forwards;
      }

      body.dark-mode .horizontal_line {
        border-top: 1px solid white;
      }

      .site-footer {
          text-align: center;
          margin-top: 30px;
          padding: 20px;
          font-family: 'Josefin Slab', serif;
          color: #333;
          background-color: #fffaf4;
      }

      .footer-line {
          border: none;
          border-top: 1px solid #aaa;
          margin: 40px auto 20px auto;
          width: 80%;
      }

      body.dark-mode .site-footer{
        background-color: #222222;
        color: white;
      }

      .home-button {
        position: absolute;
        top: 15px;
        right: 30px;
        background-color: #fffaf4;
        color: #222;
        padding: 6px 12px;
        border-radius: 6px;
        transition: background-color 0.3s, transform 0.2s;
        z-index: 10;
        margin-right: 50px;
        margin-top: 10px;
      }

      .home-button:hover {
        box-shadow: 0 12px 16px rgba(0, 0, 0, 0.24), 0 17px 50px rgba(0, 0, 0, 0.19);;
      }

      @keyframes growLine {
          from {
          width: 0;
          }
          to {
          width: 100%;
          }
      }

      body {
        margin: 0;
        padding: 0;
        overflow-x: hidden;
        overflow-y: auto;
        background-color: #fffaf4;
      }

      body.dark-mode {
        background-color: #222222;
        color: #f5f5f5;
      }

      .dark-mode-toggle {
        position: fixed;
        width: 50px;
        height: 50px;
        font-size: 20px;
        top: 15px;
        left: 30px;
        padding: 6px;
        border: none;
        border-radius: 6px;
        background-color: #f6ddd0;
        color: #000;
        cursor: pointer;
        box-shadow: 0 2px 6px rgba(0,0,0,0.2);
        transition: background-color 0.3s;

        z-index: 9999;
        pointer-events: auto;
      }

      body.dark-mode .dark-mode-toggle {
        background-color: #333;
        color: #fff;
      }

      .blog-container {
        max-width: 70%;
        margin: 50px auto 40px auto;
        padding: 10px;
        background-color: #fffaf4;
        color: #222222;
        font-family: 'Georgia', 'Times New Roman', serif;
        line-height: 1.7;
        font-size: 18px;
        border-radius: 12px;
        transition: background-color 0.4s, color 0.4s;
      }

      .blog-container h2 {
        padding-left: 0;
      }

      body.dark-mode .blog-container {
          background-color: #222222;
          color: #eaeaea;
      }

      .blog-image {
        display: flex;
        justify-content: center;
        margin: 20px 0;
      }

      .blog-image img {
        max-width: 700px;
        width: 100%;
        height: auto;
        border-radius: 8px;
      }

      .reference-list {
        list-style: decimal;
        margin: 20px 0;
        padding-left: 20px;
        font-size: 15px;
        line-height: 1.6;
      }

      .reference-list li {
        margin-bottom: 12px;
      }

      .reference-list a {
        color: #0066cc;
        text-decoration: none;
      }

      .reference-list a:hover {
        text-decoration: underline;
      }

      nav.sidebar {
        position: fixed;
        top: 0;
        left: 0;
        height: 100%;
        width: 220px;
        background-color: #f6ddd0;
        padding-top: 80px;
        font-family: 'Montserrat', sans-serif;
        transition: transform 0.3s ease;
        overflow-y: auto;
        z-index: 1000;
        transform: translateX(-220px);
      }

      nav.sidebar a {
        display: block;
        padding: 12px 20px;
        color: #222;
        text-decoration: none;
        transition: background 0.2s;
      }

      nav.sidebar a:hover {
        background-color: #ddd;
      }

      body.dark-mode nav.sidebar {
        background-color: #333;
      }

      body.dark-mode nav.sidebar a {
        color: #f5f5f5;
      }

      nav.sidebar.open {
        transform: translateX(0);
      }


      .sidebar-toggle {
        position: fixed;
        top: 15px;
        left: 120px;
        background-color: #f6ddd0;
        color: #222;
        border: none;
        border-radius: 6px;
        font-size: 24px;
        padding: 8px 12px;
        cursor: pointer;
        z-index: 1100;
        box-shadow: 0 2px 6px rgba(0,0,0,0.2);
        transition: background-color 0.3s;
      }

      .sidebar-toggle:hover {
        background-color: #e0c9bc;
      }

      body.dark-mode .sidebar-toggle {
        background-color: #333;
        color: #fff;
      }


      body, .page-header, .site-footer, .button, .horizontal_line,.blog-container, h1, p, a {
        transition: background-color 0.5s ease, color 0.5s ease, border-color 0.5s ease;
      }

      #spin-grid {
        display: grid;
        grid-template-columns: repeat(10, 50px);
        gap: 25px;
        justify-content: center;
        margin: 30px 0;
      }

      .spin-cell {
        width: 50px;
        height: 50px;
        font-size: 20px;
        text-align: center;
        line-height: 50px;
        cursor: pointer;
        border: none;
        border-radius: 50%;
        user-select: none;
        transition: background-color 0.3s, transform 0.2s;
        background-color: #f0f0f0;
        box-shadow: 0 2px 6px rgba(0, 0, 0, 0.1);
      }

      .spin-cell:hover {
        background-color: #e0e0e0;
        transform: scale(1.1);
      }

      .spin-cell.up {
        background-color: #cce5ff;
        color: #0056b3;
      }

      .spin-cell.down {
        background-color: #f8d7da;
        color: #721c24;
      }

      .timeline {
        list-style: none;
        padding: 0;
        margin: 20px 0;
      }

      .timeline li {
        display: grid;
        grid-template-columns: 100px auto;
        align-items: start;
        margin-bottom: 12px;
      }

      .timeline .year {
        font-weight: bold;
        color: #444;
      }

      .timeline .event {
        margin-left: 10px;
        line-height: 1.5;
      }

      body.dark-mode .timeline .year {
        color: #fafbfc;
      }

      #energy-plot {
        aspect-ratio: 2;
        width: 100%;
        max-width: 800px;
        height: auto;
        border: none;
        background-color: #fffaf4;
        margin: 0 auto;
        display: block;
      }

      @media (max-width: 768px) {
        #energy-plot {
          max-width: 100%;
          aspect-ratio: auto;
        }
      }

      pre {
        background-color: #f5f5f5;   /* light background */
        color: #333;                 /* dark text */
        padding: 16px;
        border-radius: 8px;
        overflow-x: auto;
        font-family: "Fira Code", "Courier New", monospace;
        font-size: 14px;
        line-height: 1.5;
      }

      .keyword { color: #0000cc; font-weight: bold; }
      .type { color: #008080; }
      .string { color: #a31515; }
      .number { color: #098658; }
      .comment { color: #008000; font-style: italic; }

      body.dark-mode pre {
        background-color: #1e1e1e;
        color: #dcdcdc;
      }
      body.dark-mode .keyword { color: #569cd6; }
      body.dark-mode .type { color: #4ec9b0; }
      body.dark-mode .string { color: #d69d85; }
      body.dark-mode .number { color: #b5cea8; }
      body.dark-mode .comment { color: #6a9955; font-style: italic; }

      .repo-link {
        text-align: center;
        margin: 3rem 0;
      }

      .repo-link p {
        font-size: 1.1rem;
        margin-bottom: 0.5rem;
      }

      .repo-button {
        display: inline-block;
        background-color: #24292e;
        color: #fff;
        padding: 0.7rem 1.5rem;
        border-radius: 6px;
        text-decoration: none;
        font-weight: 600;
        transition: background-color 0.2s ease;
      }

      .repo-button:hover {
        background-color: #444c56;
      }

      body.dark-mode .repo-button {
        background-color: #2d333b;
      }
      body.dark-mode .repo-button:hover {
        background-color: #444c56;
      }

      .github-icon {
        width: 20px;
        height: 20px;
        vertical-align: middle;
        margin-right: 8px;
        filter: invert(1);
      }

    </style>
  </head>

  <body>
    <nav class="sidebar" id="sidebar">
      <a href="#intro">Introduction</a>
      <a href="#etym">Pseudo-Etymology</a>
      <a href="#history">History</a>
      <a href="#ising">The Ising Model</a>
      <a href="#learn">Learning</a>
      <a href="#mr">Memory Retreival</a>
      <a href="#implementation">Implementation</a>
      <a href="#references">References</a>
      <a href="#comments">Comments</a>
    </nav>
    <button id="darkModeToggle" class="dark-mode-toggle">🌙</button>
    <button class="sidebar-toggle" id="sidebarToggle">☰</button>
    <a href="../index.html" class="home-button">
        <img src="../assets/images/home_button.png" alt="Home" width="40px">
    </a> 
    <h1>Hopfield Networks</h1>
    <div class="horizontal_line"></div>
    <section id="intro">
      <div class="blog-container">
        <h2><b>Introduction</b></h2>
        Working in robotics naturally pulled me into the world of AI—because in today’s era, intelligence isn’t just about code, 
        it’s about giving machines a body to interact, learn, and act in the real world. I never followed a strict, textbook path 
        to learn machine learning; instead, I stumbled into it, explored out of curiosity, and discovered how exciting the journey could be.<br><br>

        What fascinates me most is not just how AI works today, but how these ideas first took shape—the roots of the complex architectures 
        and learning algorithms we rely on. That curiosity eventually led me to dive into <b>Associative Memory Models</b>, better known as <b>
        Hopfield Networks</b> — a concept that bridges neuroscience, physics, and AI in a way that feels both timeless and futuristic.<br><br>

        One fascinating detail worth mentioning is that <b>J.J. Hopfield</b> was jointly awarded the 2024 Nobel Prize in Physics alongside <b>
        Geoffrey E. Hinton</b>, recognized for their “<i>foundational discoveries and inventions that enable machine learning with 
        artificial neural networks.</i>”<br><br>

        The ideas I’m exploring here stand on the shoulders of that legacy. They are not just the work of a single mind but the result of decades of 
        collective effort—researchers, physicists, neuroscientists, and computer scientists—each contributing to our understanding of how the brain 
        works and how we can model intelligence artificially. Without their breakthroughs, the journey from studying neurons to building artificial
        intelligence would not have been possible.<br><br>

        To put it simply, a Hopfield Network can be thought of as a network of binary units (like little switches that are either on or off) 
        connected by weighted links. These weights are carefully set so that the network “remembers” certain stable patterns—like snapshots 
        of information stored in its memory.<br><br>

        Once trained, the network can reconstruct an entire memory from just a small fragment of it. This mirrors how our own brains work: 
        for example, catching just a faint smell of a dish might instantly bring back the memory of a specific food or even the moment you
        last had it. In this way, Hopfield Networks don’t just store data—they recreate the feeling of memory recall itself.
        <div class="blog-image">
          <img src="../assets/blog-images/Hopfield_Networks/Rene_&_bot.png" alt="Rene and Bot">
        </div>
      </div>
    </section>

    <section id=etym>
      <div class="blog-container">
        <h2><b>Pseudo-Etymology</b></h2>
        The Hopfield network in itself is described as a <i>delocalized content-addressable memory or categorizer using extensive asynchronous parallel computing</i>
        <ul>
          <li>
            <b>Delocalized</b>: The stable states (the memories) are not stored in one single spot but are spread 
            across the entire network. This means that even if part of the network is disturbed, the memory can still 
            be retrieved — much like how human memory isn’t locked in a single neuron.
          </li>
          <li>
            <b>Content-addressable</b>:Providing just a piece or a noisy version of the memory is enough to retrieve the whole thing — similar 
            to recognizing a song from just a few notes.
          </li>
          <li>
            <b>Categorizer</b>: The network is able to distinguish between different stored memories, effectively 
            classifying the inputs into the right “mental folder.”
          </li>
          <li>
            <b>Extensive</b>: The capacity of the network grows with its size — the more neurons you have, the more 
            information it can store and retrieve. It scales with the system’s resources.
          </li>
          <li>
            <b>Asynchronous</b>: Neurons don’t all update at the same time. Instead, they take turns updating 
            independently, which makes the system more stable and avoids chaotic behavior. 
            This also mirrors how biological brains often work — not everything fires at once.
          </li>
        </ul>
      </div>
    </section>

    <section id="history">
      <div class="blog-container">
        <h2><b>History</b></h2>
        <p>Here’s a quick journey through the key milestones—the discoveries and breakthroughs that paved the way for the creation of Hopfield Networks and beyond.</p>

        <ul class="timeline">
          <li>
            <span class="year">1920</span>
            <span class="event">The <b>Ising Model</b> is introduced to represent the state of magnetic materials using spins.</span>
          </li>
          <li>
            <span class="year">1943</span>
            <span class="event"><b>McCulloch Pitts neuron</b> came to help us build neural networks</span>
          </li>
          <li>
            <span class="year">1957</span>
            <span class="event"><b>Ronsenblatt</b> gives world the idea of perceptron and simulates it on an <i>IBM 704</i></span>
          </li>
          <li>
            <span class="year">1963</span>
            <span class="event"><b>Roy Glauber</b> extends the Ising Model by introducing <i>Glauber dynamics</i>, allowing it to adapt and evolve over time.</span>
          </li>
          <li>
            <span class="year">1971–72</span>
            <span class="event"><b>Amari and Nakano</b>(separately) propose modifying the weights of the <i>Ising Model</i> using <i>Hebbian Learning</i>, a model for associative memory.</span>
          </li>
          <li>
            <span class="year">1975</span>
            <span class="event">The <b>Sherrington–Kirkpatrick model</b> is introduced, a mean-field model for spin glasses characterized by disordered and competing interactions between spins.</span>
          </li>
          <li>
            <span class="year">1982</span>
            <span class="event">Hopfield applied the SK-model idea with <i>binary activation function</i> to study <b>Hopfield Networks</b></span>
          </li>
          <li>
            <span class="year">1984</span>
            <span class="event">The concept of Hopfield Networks was extended to <i>continuous activation functions</i> (this is what gave rise to the modern neural networks) </span>
          </li>
          <li>
            <span class="year">1985</span>
            <span class="event"><b>Hopfield and Tank</b> presented an application of Hopfield Neural Networks to solve the popular <i>Travelling Salesman Problem</i> </span>
          </li>
        </ul>

        <div class="blog-image">
          <img src="../assets/blog-images/Hopfield_Networks/Angry_asimov.png" alt="Rene and Bot">
        </div>
      </div>
    </section>


    <section id="ising">
      <div class="blog-container">
        <h2><b>Ising Model</b></h2>
        Just like some of today’s popular machine learning models—such as diffusion models and physics-informed neural networks 
        (PINNs)—Hopfield Networks are also grounded in physics. The goal was to create a general content-addressable memory, and in 
        his original paper, Hopfield pointed out that any physical system that naturally settles into stable configurations can serve as 
        a model.<br><br>

        One of the key inspirations was the Ising Model, introduced to describe magnetism in ferromagnetic materials. In this model, every 
        unit of the material carries a tiny “spin” (either +1 or –1). The overall energy of the system depends on how these spins are 
        arranged. When placed in a magnetic field, the spins tend to align in ways that lower the energy—pushing the system toward stability. <br><br>
        
        This is more than just theory—you can experiment with it! Below, try clicking on the spins in the interactive setup to see how 
        the system’s energy shifts. Notice how each dipole (spin) only “talks” to its neighbors, yet together they form a collective behavior. 
        That same principle of local interactions leading to global order is what Hopfield leveraged to design his networks. <br><br>
      </div>
      <div id="spin-grid"></div>
      <div class="blog-container">
        Total Energy: <span id="energy-display">0</span><br><br>
        The Ising model defines energy (without any external magnetic field) as:
        <div>
          $$ E = -\sum_{\langle i, j \rangle}J_{i, j}\sigma_{i}\sigma_{j} $$
        </div>
        <ul>
          <li>\( J_{i, j} \) is the interaction between the dipoles</li>
          <li>\( \sigma_{i} \) is the spin value (either 1 or -1)</li>
        </ul>
        The central idea behind Hopfield Networks is the concept of energy minimization. A memory is introduced as input, and 
        the network adjusts its interaction parameters so that the overall energy of the system is reduced. Stable states, which 
        represent stored memories, correspond to these minimum-energy configurations.<br><br>

        This principle effectively serves as the loss function for the network. Structurally, Hopfield Networks borrow from the 
        Ising Model: a graph of interconnected binary units (0 or 1), where the state of each unit influences its neighbors. Through these 
        interactions, the network converges toward a stable pattern that encodes the memory.<br><br>

        Shown below is a 3D plot of example energy function. The minima will correspond to the stable memory location. So if the network 
        was queried at a region near a particular minimum (i.e. partial memory), it would eventually converge to a local minimum to give
        the full memory as the output.

        <div align="center">
          <script type="module" src="../javascript/energy.js"></script>
          <canvas id="energy-plot"></canvas>
        </div>
      </div>
    </section>

    <section id="learn">
      <div class="blog-container">
        <h2><b>Learning</b></h2>
        Now that we have defined an energy-based loss function, we need a method to optimize the network parameters so that the system can learn effectively. 
        Several approaches exist, but here we focus on one of the most widely recognized methods.  
        It is important to note that Hopfield Networks employ <b>symmetric weights</b> 
        (the weight from neuron <i>i</i> to <i>j</i> is the same as from <i>j</i> to <i>i</i>) to guarantee the convergence of energy values.
        
        <h3>Hebbian Learning</h3>
        <p>
        Donald Hebb introduced the <b>Hebbian theory</b>, which describes how the simultaneous activation of two neurons strengthens the synaptic connection between them.  
        This is often summarized as: <i>“neurons that fire together, wire together.”</i>  
        In the context of Hopfield Networks, this principle is used to update the weights of the network.  
        The weight for each connection is calculated as:
        </p>
        
        <div>
          $$ w_{i,j} = \frac{1}{n} \sum_{\mu=1}^{n} (2 \epsilon_{i}^{\mu} - 1) \, (2\epsilon_{j}^{\mu}-1) $$
        </div>
        <ul>
          <li>\( w_{i,j} \) is the weight of the connection between neuron <i>i</i> and neuron <i>j</i></li>
          <li>\( n \) is the total number of stored patterns</li>
          <li>\( \mu \) is the index of a specific memory pattern</li>
          <li>\( \epsilon_{i}^{\mu} \) is the state of neuron <i>i</i> in memory pattern <i>μ</i></li>
          <li>\( \epsilon_{i}^{\mu} \) is the state of neuron <i>j</i> in memory pattern <i>μ</i></li>
        </ul>

        Let’s imagine we have just two simple neurons. From the formula, you can see that:

        <ul>
          <li>If both neurons are in the same state (either (1, 1) or (0, 0)), the connection between them — the weight — is positive.</li>
          <li>If the neurons are in opposite states ((1, 0) or (0, 1)), the weight becomes negative.</li>
        </ul>
        This clever setup ensures that the system always prefers lower energy levels, pulling it toward stable points (minima). In 
        other words, the weights are chosen so that each possible state naturally settles into one of the energy valleys, just like a 
        ball rolling down into the lowest part of a hill.
        

      </div>
    </section>

    <section id="mr">
      <div class="blog-container">
        <h2><b>Memory Retrieval</b></h2>
        <p>
          We’ve seen how the network learns to store memories using the mathematical framework discussed earlier. 
          But how do we actually <em>retrieve</em> a stored memory once the network is trained?
        </p>
    
        <p>
          Hopfield, in his original paper, proposed a binary activation update rule for neurons. 
          The idea is simple: if we provide a <strong>partial or noisy memory</strong> as input, 
          each neuron updates its state based on the weighted contributions from all the other connected neurons. 
          If this summed input crosses a certain threshold, the neuron activates (<code>1</code>); otherwise, it deactivates (<code>0</code>).
        </p>
    
        <p>The update rule can be written as:</p>
        <div>
          $$ 
          \sigma_i =
          \begin{cases}
          1, & \text{if } \sum_{j} w_{i,j}\sigma_j - \theta \ge 0 \\
          0, & \text{otherwise}
          \end{cases}
          $$
        </div>
    
        <p>
          Where:
          <ul>
            <li>\( \sigma_i \) is the binary state of neuron \( i \) (either 0 or 1)</li>
            <li>\( w_{i,j} \) is the learned weight between neuron \( i \) and \( j \)</li>
            <li>\( \theta \) is the threshold value</li>
            <li>The summation is over all neurons \( j \) connected to \( i \)</li>
          </ul>
        </p>
    
        <p>
          By repeatedly applying this rule, the network gradually converges from the given partial input 
          to the closest stored memory pattern — effectively completing the memory.
        </p>
      </div>
    </section>
    

    <section id="implementation">
      <div class="blog-container">
        <h2><b>Implementation</b></h2>
        <p>
          The <code>Hnet</code> class below represents a basic Hopfield Network model in C++. 
          It stores the connection strengths between neurons (the <b>weights</b>), 
          keeps track of the <b>number of neurons</b> (<code>n</code>), and uses a <b>threshold</b> 
          value to determine activation.  
          The main components are:
          </p>
          <ul>
            <li><code>weights</code>: A 2D matrix of doubles representing connections between neurons.</li>
            <li><code>n</code>: The total number of neurons in the network.</li>
            <li><code>threshold</code>: The activation threshold for neuron state changes (default is 0.0).</li>
          </ul>
          <p>
          The class provides methods to:
          </p>
          <ul>
            <li><code>energy</code>: Calculate the current energy of the network given a state.</li>
            <li><code>learn</code>: Update weights using a chosen learning method and training states.</li>
            <li><code>infer</code>: Evolve the network state until it stabilizes (or until max iterations).</li>
            <li><code>save_weights</code> / <code>load_weights</code>: Store and retrieve trained weights from a file.</li>
          </ul>
          
        <pre><code>
          <span class="keyword">class</span> Hnet {
          <span class="keyword">private</span>:
              std::vector&lt;std::vector&lt;<span class="type">double</span>&gt;&gt; weights;
              <span class="type">int</span> n;
              <span class="type">double</span> threshold = <span class="number">0.0</span>;
          
          <span class="keyword">public</span>:
              Hnet(<span class="type">int</span> n);
              Hnet(<span class="type">int</span> n, <span class="type">double</span> threshold);
          
              <span class="type">double</span> energy(<span class="keyword">const</span> std::vector&lt;<span class="type">int</span>&gt;&amp; state);
              <span class="type">void</span> learn(Learning_method lm, <span class="keyword">const</span> std::vector&lt;std::vector&lt;<span class="type">int</span>&gt;&gt;&amp; states);
              <span class="type">void</span> infer(std::vector&lt;<span class="type">int</span>&gt;&amp; state, <span class="type">int</span> max_iters = <span class="number">100</span>);
              <span class="type">void</span> save_weights(<span class="keyword">const</span> std::string&amp; filename) <span class="keyword">const</span>;
              <span class="type">void</span> load_weights(<span class="keyword">const</span> std::string&amp; filename);
          };
        </code></pre>
        <p>
          The <code>learn</code> function is where the Hopfield Network actually <b>trains</b>.  
          It uses the <b>Hebbian learning rule</b> to update the weights based on training states.
        </p>
        
        <ul>
          <li><code>Learning method check</code>: If the method is not Hebbian, it simply returns without doing anything.</li>
          <li><code>Hebbian update</code>: For every pair of neurons <code>(j, k)</code>, the weight is updated using the rule 
            <code>(2*state[j] - 1) * (2*state[k] - 1)</code>, which reinforces same states and penalizes opposite ones.</li>
          <li><code>Weight merge</code>: After all threads finish, their local weights are combined into the main weight matrix.</li>
        </ul>
        
        <p>
          In short: This function takes the training states, applies Hebbian learning in parallel, and updates the network’s weights so that 
          the given patterns become stable energy minima.
        </p>

        <pre><code>
          <span class="type">void</span> Hnet::learn(Learning_method lm, <span class="keyword">const</span> std::vector&lt;std::vector&lt;<span class="type">int</span>&gt;&gt;&amp; states) {
              <span class="keyword">if</span> (lm != Hebbian) <span class="keyword">return</span>;
          
              <span class="type">int</span> total_states = states.size();
          
              <span class="keyword">for</span> (<span class="type">int</span> idx = <span class="number">0</span>; idx &lt; total_states; idx++) {
                  <span class="keyword">const</span> <span class="keyword">auto</span>&amp; state = states[idx];
          
                  <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; n; j++) {
                      <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; n; k++) {
                          <span class="keyword">if</span> (j == k) <span class="keyword">continue</span>;
                          weights[j][k] += (2 * state[j] - 1) * (2 * state[k] - 1);
                      }
                  }
              }
          }
        </code></pre>
        <p>
        Starting from an <b>initial (possibly incomplete or noisy) state</b>, the <code>infer</code> function repeatedly updates each 
        neuron based on the weighted input from all other neurons until the network reaches a stable state or the maximum number of 
        iterations is reached. Each neuron's new state is computed using the net weighted sum of all other neurons. The process stops 
        early if the state stops changing (converges).
        </p>

        <pre><code>
          <span class="type">void</span> Hnet::infer(std::vector&lt;<span class="type">int</span>&gt;&amp; incomplete_state, <span class="type">int</span> max_iters) {
              <span class="keyword">for</span> (<span class="type">int</span> iter = <span class="number">0</span>; iter &lt; max_iters; iter++) {
                  std::vector&lt;<span class="type">int</span>&gt; next_state(n);
          
                  <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) {
                      <span class="type">double</span> net_input = <span class="number">0.0</span>;
          
                      <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; n; j++) {
                          <span class="keyword">if</span> (i == j) <span class="keyword">continue</span>;
                          net_input += weights[i][j] * (2 * incomplete_state[j] - 1);
                      }
          
                      net_input -= threshold;
                      next_state[i] = net_input &gt;= <span class="number">0</span> ? <span class="number">1</span> : <span class="number">0</span>;
                  }
          
                  <span class="keyword">if</span> (next_state == incomplete_state) <span class="keyword">break</span>;
                  incomplete_state.swap(next_state);
              }
          }
        </code></pre>

        You can check out the full implementation of this code with parallelization performed on the <b>MNIST handwritten digit recognition</b> dataset 
        here. My stupid-ass just tried training it on all the images even though Hopfield mentioned about the <b>0.15N soft limit</b>. So all I get 
        is some random spurious pattern. Also one thing to mention is I found a paper where they attempted H-Net on the MNIST dataset and they
        mentioned the Hebbian method doesn't work well so I will try implementing the Storkey method on this.
        <div class="repo-link">
          <a href="https://github.com/username/project-name" target="_blank" class="repo-button">
            <img src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons/icons/github.svg" alt="GitHub" class="github-icon">
            View on GitHub
          </a>
        </div>
      </div>
    </section>

    <section id="references">
      <div class="blog-container">
        <h2><b>References</b></h2>
        <ol class="reference-list">
          <li>
            Hopfield, J. J. (1982). <i>Neural networks and physical systems with emergent collective 
            computational abilities.</i> Proceedings of the National Academy of Sciences, 79(8), 2554–2558. 
            <a href="https://doi.org/10.1073/pnas.79.8.2554" target="_blank">https://doi.org/10.1073/pnas.79.8.2554</a>
          </li>

          <li>
            McCulloch, W. S., & Pitts, W. (1943). <i>A logical calculus of the ideas immanent in nervous activity.</i> 
            The Bulletin of Mathematical Biophysics, 5(4), 115–133. 
            <a href="https://doi.org/10.1007/BF02478259" target="_blank">https://doi.org/10.1007/BF02478259</a>
          </li>
        
          <li>
            Bruck, J. (1990). <i>On the convergence properties of the Hopfield model.</i> 
            Proceeings of the IEEE, 78(10), 1579–1585. 
            <a href="https://doi.org/10.1109/5.58341" target="_blank">https://doi.org/10.1109/5.58341</a>
          </li>
        
          <li>
            Storkey, A. (1997). <i>Increasing the capacity of a Hopfield network without sacrificing functionality.</i> 
            In W. Gerstner, A. Germond, M. Hasler, & J.-D. Nicoud (Eds.), 
            <i>Artificial Neural Networks — ICANN’97</i> (Lecture Notes in Computer Science, vol 1327, pp. 451–456). 
            Springer, Berlin, Heidelberg. 
            <a href="https://doi.org/10.1007/BFb0020196" target="_blank">https://doi.org/10.1007/BFb0020196</a>
          </li>
        
          <li>
            Uykan, Z. (2020). <i>On the working principle of the Hopfield Neural Networks and its equivalence to the GADIA in optimization.</i> 
            IEEE Transactions on Neural Networks and Learning Systems, 31(9), 3294–3304. 
            <a href="https://doi.org/10.1109/TNNLS.2019.2940920" target="_blank">https://doi.org/10.1109/TNNLS.2019.2940920</a>
          </li>
        
          <li>
            Hopfield, J. J., & Tank, D. W. (1985). <i>Computing with neural circuits: A model.</i> 
            Science, 233(4764), 625–633. 
            <a href="https://doi.org/10.1126/science.3755256" target="_blank">https://doi.org/10.1126/science.3755256</a>
          </li>
        </ol>
      </div>
    </section>

    <section id="links">
      <div class="blog-container">
        <h2><b>Other links</b></h2>
        <ol class="reference-list">
          <li><a href="https://youtu.be/1WPJdAW-sFo?si=yx-XRzFIWPwG__jC" target="_blank">A Brain-Inspired Algorithm For Memory - Artem Kirsanov</a></li>
          <li><a href="https://youtu.be/piF6D6CQxUw?si=Kn8rutb11ygu2q7J" target="_blank">Hopfield network: How are memories stored in neural networks? [Nobel Prize in Physics 2024] - Layerwise Lectures</a></li>
          <li><a href="https://www.geeksforgeeks.org/machine-learning/hopfield-neural-network/" target="_blank">Hopfield Neural Network - GeeksforGeeks</a></li>
          <li><a href="https://towardsdatascience.com/hopfield-networks-neural-memory-machines-4c94be821073/" target="_blank">Hopfield Networks: Neural Memory Machines</a></li>
        </ol>
      </div>
    </section>


    <section id="comments">
      <h2>Comments</h2>
      <script src="https://giscus.app/client.js"
          data-repo="aayush-rath/aayush-rath.github.io"
          data-repo-id="R_kgDOPGW6yA"
          data-category="Announcements"
          data-category-id="DIC_kwDOPGW6yM4CtOsv"
          data-mapping="pathname"
          data-strict="0"
          data-reactions-enabled="1"
          data-emit-metadata="0"
          data-input-position="bottom"
          data-theme="preferred_color_scheme"
          data-lang="en"
          crossorigin="anonymous"
          async>
      </script>
    </section>

    <footer class="site-footer">
      <hr class="footer-line">
      <p>&copy; 2025 Aayush Rath. All rights reserved.</p>
    </footer>

    <script>
        const toggleBtn = document.getElementById('darkModeToggle');
        
        const isDarkMode = () => document.body.classList.contains('dark-mode');
        
        function updateGiscusTheme(isDark) {
            const giscusFrame = document.querySelector('iframe.giscus-frame');
            if (!giscusFrame) return;
            const message = {
            setConfig: {
                theme: isDark ? 'dark' : 'light'
            }
            };
            giscusFrame.contentWindow.postMessage(message, 'https://giscus.app');
        }
        
        function applyDarkModePreference() {
            const enabled = localStorage.getItem('darkMode') === 'enabled';
            document.body.classList.toggle('dark-mode', enabled);
        
            toggleBtn.textContent = enabled ? '🌞' : '🌙';
        
            if (window.updateThreeSceneForDarkMode)
            window.updateThreeSceneForDarkMode(enabled);
        
            updateGiscusTheme(enabled);
        }
        
        toggleBtn.addEventListener('click', () => {
            const enabled = !isDarkMode();
            document.body.classList.toggle('dark-mode', enabled);
            localStorage.setItem('darkMode', enabled ? 'enabled' : 'disabled');
            toggleBtn.textContent = enabled ? '🌞' : '🌙';
        
            if (window.updateThreeSceneForDarkMode)
            window.updateThreeSceneForDarkMode(enabled);
        
            updateGiscusTheme(enabled);
        });
        
        document.addEventListener('sceneReady', applyDarkModePreference);
        applyDarkModePreference();

        const sidebar = document.getElementById("sidebar");
        const sidebarToggle = document.getElementById("sidebarToggle");

        sidebarToggle.addEventListener("click", () => {
          sidebar.classList.toggle("open");
        });

        document.querySelectorAll('.sidebar a').forEach(anchor => {
          anchor.addEventListener('click', function(e) {
            e.preventDefault();
            document.querySelector(this.getAttribute('href')).scrollIntoView({
              behavior: 'smooth'
            });
            sidebar.classList.remove("open");
          });
        });
    </script>      
    <script>
        const N = 6;
        let spins = Array.from({ length: N }, () => Array(N).fill(1));

        const grid = document.getElementById('spin-grid');
        const energyDisplay = document.getElementById('energy-display');

        function createGrid() {
            grid.innerHTML = '';
            grid.style.gridTemplateColumns = `repeat(${N}, 35px)`;
            for (let i = 0; i < N; i++) {
                for (let j = 0; j < N; j++) {
                    const cell = document.createElement('div');
                    cell.classList.add('spin-cell', spins[i][j] === 1 ? 'up' : 'down');
                    cell.dataset.row = i;
                    cell.dataset.col = j;
                    cell.textContent = spins[i][j] === 1 ? '↑' : '↓';
                    cell.addEventListener('click', toggleSpin);
                    grid.appendChild(cell);
                }
            }
            updateEnergy();
        }

        function toggleSpin(e) {
            const i = parseInt(e.target.dataset.row);
            const j = parseInt(e.target.dataset.col);
            spins[i][j] *= -1;
            createGrid();
        }

        function updateEnergy() {
            let E = 0;
            for (let i = 0; i < N; i++) {
                for (let j = 0; j < N; j++) {
                    const s = spins[i][j];
                    if (i > 0) E -= s * spins[i - 1][j];
                    if (j > 0) E -= s * spins[i][j - 1];
                    if (i < N - 1) E -= s * spins[i + 1][j];
                    if (j < N - 1) E -= s * spins[i][j + 1];
                }
            }
            energyDisplay.textContent = E / 2;
        }

        createGrid();
    </script>
  </body>
</html>